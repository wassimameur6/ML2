# ğŸ¯ DÃ©tection de Fraude Bancaire - ModÃ©lisation 

**Projet MLOps - Fine-Tuning et Ensemble Learning pour la DÃ©tection de Fraudes**

> Pipeline complet de modÃ©lisation avancÃ©e avec optimisation d'hyperparamÃ¨tres (RandomizedSearchCV) et mÃ©thodes d'ensemble (Stacking, Voting) pour maximiser les performances.

---

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'Ensemble](#-vue-densemble)
- [Structure du Projet](#-structure-du-projet)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Pipeline Complet](#-pipeline-complet)
- [RÃ©sultats](#-rÃ©sultats)
- [Architecture](#-architecture)
- [DÃ©ploiement](#-dÃ©ploiement)
- [Contribuer](#-contribuer)

---

## ğŸŒŸ Vue d'Ensemble

### Objectif

DÃ©velopper un systÃ¨me de dÃ©tection de fraude bancaire hautement performant en utilisant:
- **Fine-tuning automatisÃ©** avec RandomizedSearchCV
- **Ensemble Learning** (Stacking et Voting)
- **Comparaison multi-niveaux** (Baseline vs Tuned vs Ensemble)

### ModÃ¨les ImplÃ©mentÃ©s

| ModÃ¨le | Type | Performances Attendues |
|--------|------|------------------------|
| **Random Forest** | Ensemble | ROC-AUC: 0.94-0.97 |
| **XGBoost** | Gradient Boosting | ROC-AUC: 0.95-0.98 |
| **LightGBM** | Gradient Boosting | ROC-AUC: 0.94-0.97 |
| **CatBoost** | Gradient Boosting | ROC-AUC: 0.95-0.97 |
| **Stacking** | Meta-Ensemble | ROC-AUC: 0.96-0.98 |
| **Voting** | Soft Ensemble | ROC-AUC: 0.95-0.97 |

### Pipeline en 3 Phases

```mermaid
graph LR
    A[Phase 1: Baseline] --> B[Phase 2: Fine-Tuning]
    B --> C[Phase 3: Stacking]
    C --> D[Meilleur ModÃ¨le]
```

1. **Phase 1**: EntraÃ®nement des modÃ¨les avec paramÃ¨tres par dÃ©faut
2. **Phase 2**: Optimisation des hyperparamÃ¨tres (RandomizedSearchCV)
3. **Phase 3**: CrÃ©ation d'ensembles (Stacking + Voting)

---

## ğŸ“ Structure du Projet

```
projet/
â”‚
â”œâ”€â”€ README.md                          # â† Ce fichier
â”œâ”€â”€ requirements.txt                   # â† DÃ©pendances Python (racine)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fraud.csv                      # DonnÃ©es brutes
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ fraud_detection_modeling.ipynb  # â† Notebook principal
â”‚   â””â”€â”€ processors/                    # GÃ©nÃ©rÃ© par preprocessing et notebook
â”‚       â”œâ”€â”€ preprocessed_data.pkl      # DonnÃ©es preprocessÃ©es
â”‚       â”œâ”€â”€ scaler.pkl                 # RobustScaler
â”‚       â”œâ”€â”€ label_encoders.pkl         # Encodeurs
â”‚       â”œâ”€â”€ feature_names.pkl          # Noms des features
â”‚       â”œâ”€â”€ smote_config.pkl           # Configuration SMOTE
â”‚       â”œâ”€â”€ models/                    # ModÃ¨les sauvegardÃ©s
â”‚       â”‚   â”œâ”€â”€ best_model_final.pkl   # Meilleur modÃ¨le global
â”‚       â”‚   â”œâ”€â”€ best_model_final_metadata.pkl
â”‚       â”‚   â”œâ”€â”€ all_tuned_models.pkl   # Tous les modÃ¨les tunÃ©s
â”‚       â”‚   â””â”€â”€ ensemble_models.pkl    # Stacking + Voting
â”‚       â”œâ”€â”€ model_comparison_final.csv # Tableau comparatif
â”‚       â”œâ”€â”€ model_improvements.csv     # Analyse des gains
â”‚       â””â”€â”€ modeling_report_final.txt  # Rapport dÃ©taillÃ©
â”‚
â””â”€â”€ backend/
    â””â”€â”€ src/
        â”œâ”€â”€ preprocessing_fraud_class.py  # Classe de preprocessing
        â””â”€â”€ model_inference.py            # â† Script d'infÃ©rence (production)
```

### ğŸ“¦ Fichiers ClÃ©s

| Fichier | Description | Localisation |
|---------|-------------|--------------|
| `requirements.txt` | DÃ©pendances Python | **Racine** |
| `fraud_detection_modeling.ipynb` | Notebook principal | `notebooks/` |
| `model_inference.py` | Script d'infÃ©rence | **`backend/src/`** |
| `preprocessing_fraud_class.py` | Preprocessing | `backend/src/` |
| `best_model_final.pkl` | Meilleur modÃ¨le | `notebooks/processors/models/` |

---

## ğŸš€ Installation

### 1ï¸âƒ£ PrÃ©requis

- Python 3.11 ou supÃ©rieur
- pip (gestionnaire de packages)
- 8-16 GB RAM disponible
- 10-20 GB espace disque

### 2ï¸âƒ£ Installer les DÃ©pendances

```bash
# CrÃ©er un environnement conda (recommandÃ©)
conda create -n mlops python=3.11 anaconda 

# Activer l'environnement
conda activate mlops

# Installer les dÃ©pendances depuis la racine
pip install -r requirements.txt
```

**Note**: Le fichier `requirements.txt` est Ã  la **racine** du projet.

### 3ï¸âƒ£ VÃ©rifier l'Installation

```bash
python -c "import pandas, sklearn, xgboost, lightgbm, catboost; print('âœ… Tout est installÃ©!')"
```

---

## ğŸ’» Utilisation

### Ã‰tape 1: Preprocessing (PrÃ©requis)

**âš ï¸ IMPORTANT**: Avant d'exÃ©cuter le notebook, le preprocessing doit Ãªtre effectuÃ©.

```bash
cd backend/src/
python preprocessing_fraud_class.py
```

Cela crÃ©era le dossier `notebooks/processors/` avec:
- `preprocessed_data.pkl`
- `scaler.pkl`
- `label_encoders.pkl`
- `feature_names.pkl`

### Ã‰tape 2: Lancer le Notebook

```bash
# Depuis la racine du projet
cd notebooks/
jupyter notebook fraud_detection_modeling.ipynb
```

### Ã‰tape 3: ExÃ©cuter le Notebook

Dans Jupyter:
1. Ouvrir `fraud_detection_modeling.ipynb`
2. Menu: **Cell â†’ Run All**
3. â³ Attendre 20-30 minutes (fine-tuning inclus)

### Configuration Rapide vs ComplÃ¨te

#### âš¡ Mode Rapide (Test)
Dans la cellule 5.2, modifier:
```python
tuned_models, best_params = fine_tune_models(
    baseline_models,
    param_distributions,
    X_train,
    y_train,
    n_iter=5,   # â† RÃ©duire Ã  5 pour test rapide
    cv=2        # â† RÃ©duire Ã  2
)
```
**Temps**: ~5-10 minutes

#### ğŸ† Mode Production (Optimal)
```python
tuned_models, best_params = fine_tune_models(
    baseline_models,
    param_distributions,
    X_train,
    y_train,
    n_iter=50,  # â† Augmenter Ã  50 pour meilleurs rÃ©sultats
    cv=5        # â† Augmenter Ã  5 pour validation robuste
)
```
**Temps**: ~45-60 minutes

---

## ğŸ”„ Pipeline Complet

### Vue d'Ensemble

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE COMPLET                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. PREPROCESSING                                               â”‚
â”‚     â””â”€ backend/src/preprocessing_fraud_class.py                â”‚
â”‚        â€¢ Nettoyage des donnÃ©es                                  â”‚
â”‚        â€¢ Feature engineering                                    â”‚
â”‚        â€¢ Encodage et normalisation                              â”‚
â”‚        â€¢ SMOTE pour Ã©quilibrage                                 â”‚
â”‚        â€¢ Sauvegarde: notebooks/processors/                     â”‚
â”‚                                                                 â”‚
â”‚  2. MODÃ‰LISATION BASELINE                                       â”‚
â”‚     â””â”€ notebooks/fraud_detection_modeling.ipynb       â”‚
â”‚        â€¢ Random Forest                                          â”‚
â”‚        â€¢ XGBoost                                                â”‚
â”‚        â€¢ LightGBM                                               â”‚
â”‚        â€¢ CatBoost                                               â”‚
â”‚        â€¢ RÃ©sultats: ROC-AUC ~0.94                              â”‚
â”‚                                                                 â”‚
â”‚  3. FINE-TUNING (RandomizedSearchCV)                           â”‚
â”‚     â””â”€ Optimisation automatique                                 â”‚
â”‚        â€¢ 20 itÃ©rations Ã— 3 CV par modÃ¨le                       â”‚
â”‚        â€¢ 5-8 hyperparamÃ¨tres optimisÃ©s                         â”‚
â”‚        â€¢ SÃ©lection des meilleurs paramÃ¨tres                     â”‚
â”‚        â€¢ RÃ©sultats: ROC-AUC ~0.96 (+2%)                        â”‚
â”‚                                                                 â”‚
â”‚  4. ENSEMBLE LEARNING                                           â”‚
â”‚     â””â”€ Stacking + Voting                                        â”‚
â”‚        â€¢ Stacking: 4 modÃ¨les + meta-learner                    â”‚
â”‚        â€¢ Voting: Soft voting sur probabilitÃ©s                   â”‚
â”‚        â€¢ RÃ©sultats: ROC-AUC ~0.97 (+3%)                        â”‚
â”‚                                                                 â”‚
â”‚  5. Ã‰VALUATION ET COMPARAISON                                   â”‚
â”‚     â””â”€ Analyses complÃ¨tes                                       â”‚
â”‚        â€¢ 10 configurations comparÃ©es                            â”‚
â”‚        â€¢ Matrices de confusion                                  â”‚
â”‚        â€¢ Courbes ROC                                            â”‚
â”‚        â€¢ Analyse des amÃ©liorations                              â”‚
â”‚                                                                 â”‚
â”‚  6. SAUVEGARDE                                                  â”‚
â”‚     â””â”€ notebooks/processors/models/                            â”‚
â”‚        â€¢ best_model_final.pkl                                   â”‚
â”‚        â€¢ all_tuned_models.pkl                                   â”‚
â”‚        â€¢ ensemble_models.pkl                                    â”‚
â”‚        â€¢ MÃ©tadonnÃ©es et rapports                                â”‚
â”‚                                                                 â”‚
â”‚  7. INFÃ‰RENCE (PRODUCTION)                                      â”‚
â”‚     â””â”€ backend/src/model_inference.py                          â”‚
â”‚        â€¢ Chargement du meilleur modÃ¨le                          â”‚
â”‚        â€¢ PrÃ©dictions sur nouvelles transactions                 â”‚
â”‚        â€¢ API-ready                                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Commandes SÃ©quentielles

```bash
# 1. Preprocessing
cd backend/src/
python preprocessing_fraud_class.py

# 2. ModÃ©lisation
cd ../../notebooks/
jupyter notebook fraud_detection_modeling.ipynb
# â†’ ExÃ©cuter toutes les cellules

# 3. VÃ©rifier les rÃ©sultats
ls -l processors/models/
cat processors/modeling_report_final.txt

# 4. Tester l'infÃ©rence
cd ../backend/src/
python model_inference.py
```

---

## ğŸ“Š RÃ©sultats

### Performances Typiques

| Phase | ROC-AUC | F1-Score | Recall | Temps |
|-------|---------|----------|--------|-------|
| **Baseline** | 0.94 | 0.87 | 0.85 | 2 min |
| **Fine-Tuned** | 0.96 | 0.90 | 0.89 | 20 min |
| **Ensemble** | 0.97 | 0.92 | 0.91 | 25 min |

### AmÃ©lioration Globale

```
Baseline â†’ Ensemble
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROC-AUC:  0.94 â†’ 0.97  (+3.2%)
F1-Score: 0.87 â†’ 0.92  (+5.7%)
Recall:   0.85 â†’ 0.91  (+7.1%)
```

### Fichiers GÃ©nÃ©rÃ©s

AprÃ¨s exÃ©cution complÃ¨te:

```
notebooks/processors/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model_final.pkl           (2-5 MB)
â”‚   â”œâ”€â”€ best_model_final_metadata.pkl  (10 KB)
â”‚   â”œâ”€â”€ all_tuned_models.pkl           (8-20 MB)
â”‚   â””â”€â”€ ensemble_models.pkl            (10-25 MB)
â”œâ”€â”€ model_comparison_final.csv         (5 KB)
â”œâ”€â”€ model_improvements.csv             (2 KB)
â””â”€â”€ modeling_report_final.txt          (10 KB)
```

### Exemple de Rapport

```
ğŸ† MEILLEUR MODÃˆLE GLOBAL: Stacking
   Type: Ensemble
   ROC-AUC:   0.9745
   F1-Score:  0.9234
   Recall:    0.9156
   Precision: 0.9314

ğŸ“Š TOP 5 MODÃˆLES (PAR ROC-AUC)
1. Stacking (Ensemble) - AUC: 0.9745
2. XGBoost (Tuned) - AUC: 0.9687
3. LightGBM (Tuned) - AUC: 0.9654
4. CatBoost (Tuned) - AUC: 0.9623
5. Random Forest (Tuned) - AUC: 0.9598

ğŸ’¡ AMÃ‰LIORATION GLOBALE
   Baseline â†’ Ensemble: +3.2% ROC-AUC
```

---

## ğŸ—ï¸ Architecture

### Technologies UtilisÃ©es

#### Core ML
- **Scikit-learn** 1.0+: Framework ML principal
- **XGBoost** 1.5+: Gradient boosting optimisÃ©
- **LightGBM** 3.3+: Gradient boosting rapide
- **CatBoost** 1.0+: Gradient boosting pour catÃ©gories

#### Data Processing
- **Pandas** 1.3+: Manipulation de donnÃ©es
- **NumPy** 1.21+: Calcul numÃ©rique
- **Imbalanced-learn** 0.9+: SMOTE pour dÃ©sÃ©quilibre

#### Visualization
- **Matplotlib** 3.4+: Graphiques
- **Seaborn** 0.11+: Visualisations statistiques

#### Optimization
- **Scipy** 1.7+: Distributions pour RandomizedSearchCV

### Architecture du ModÃ¨le Final

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           STACKING ENSEMBLE (Meilleur)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  BASE LEARNERS (Level 0)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Random      â”‚  â”‚  XGBoost    â”‚              â”‚
â”‚  â”‚ Forest      â”‚  â”‚  (Tuned)    â”‚              â”‚
â”‚  â”‚ (Tuned)     â”‚  â”‚             â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                 â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  LightGBM   â”‚  â”‚  CatBoost   â”‚              â”‚
â”‚  â”‚  (Tuned)    â”‚  â”‚  (Tuned)    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                 â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                  â–¼                              â”‚
â”‚  META-LEARNER (Level 1)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  Logistic Regression    â”‚                    â”‚
â”‚  â”‚  (Balanced)             â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚              â”‚                                  â”‚
â”‚              â–¼                                  â”‚
â”‚         PREDICTION                              â”‚
â”‚         (0 ou 1)                                â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ DÃ©ploiement

### Utilisation du Script d'InfÃ©rence

Le script `model_inference.py` est dans **`backend/src/`** et est prÃªt pour la production.

#### Import et Initialisation

```python
# backend/src/model_inference.py
from model_inference import FraudDetectionInference

# Initialiser le dÃ©tecteur
detector = FraudDetectionInference(
    processor_dir='../../notebooks/processors'
)

# Charger le meilleur modÃ¨le
detector.load_model()
```

#### PrÃ©diction sur une Transaction

```python
# Transaction unique
transaction = {
    'amt': 125.50,
    'lat': 40.7128,
    'long': -74.0060,
    'city_pop': 8000000,
    # ... autres features
}

result = detector.predict_single(transaction)
print(f"Fraude: {result['is_fraud']}")
print(f"ProbabilitÃ©: {result['fraud_probability']:.2%}")
print(f"Niveau de risque: {result['risk_level']}")
```

**Sortie**:
```
Fraude: 1
ProbabilitÃ©: 87.34%
Niveau de risque: TRÃˆS Ã‰LEVÃ‰
```

#### PrÃ©diction sur un Batch

```python
# Batch de transactions
import pandas as pd

transactions_df = pd.read_csv('new_transactions.csv')

# PrÃ©dictions avec dÃ©tails
results_df = detector.predict_batch(
    transactions_df,
    threshold=0.5,
    return_details=True
)

# Filtrer les fraudes dÃ©tectÃ©es
frauds = results_df[results_df['is_fraud_predicted'] == 1]
print(f"Fraudes dÃ©tectÃ©es: {len(frauds)}")
```

### IntÃ©gration dans une API Flask

```python
# backend/src/api.py
from flask import Flask, request, jsonify
from model_inference import FraudDetectionInference

app = Flask(__name__)
detector = FraudDetectionInference()
detector.load_model()

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    result = detector.predict_single(data)
    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### IntÃ©gration dans une API FastAPI

```python
# backend/src/api_fast.py
from fastapi import FastAPI
from pydantic import BaseModel
from model_inference import FraudDetectionInference

app = FastAPI()
detector = FraudDetectionInference()
detector.load_model()

class Transaction(BaseModel):
    amt: float
    lat: float
    long: float
    # ... autres champs

@app.post("/predict")
async def predict(transaction: Transaction):
    result = detector.predict_single(transaction.dict())
    return result
```

### Dockerisation

```dockerfile
# Dockerfile
FROM python:3.8-slim

WORKDIR /app

# Copier requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code
COPY backend/ backend/
COPY notebooks/processors/ notebooks/processors/

# Exposer le port
EXPOSE 5000

# Lancer l'API
CMD ["python", "backend/src/api.py"]
```

### Tests Unitaires

```python
# tests/test_inference.py
import unittest
from backend.src.model_inference import FraudDetectionInference

class TestInference(unittest.TestCase):
    
    @classmethod
    def setUpClass(cls):
        cls.detector = FraudDetectionInference()
        cls.detector.load_model()
    
    def test_load_model(self):
        self.assertIsNotNone(self.detector.model)
    
    def test_predict_single(self):
        transaction = {...}  # Transaction test
        result = self.detector.predict_single(transaction)
        self.assertIn('is_fraud', result)
        self.assertIn('fraud_probability', result)
```

---

## ğŸ“ˆ Monitoring et Maintenance

### MÃ©triques Ã  Suivre

| MÃ©trique | Seuil Critique | Action |
|----------|----------------|--------|
| ROC-AUC | < 0.90 | RÃ©entraÃ®ner |
| Recall | < 0.85 | Ajuster seuil |
| Precision | < 0.80 | RÃ©viser features |
| Temps infÃ©rence | > 100ms | Optimiser |

### RÃ©entraÃ®nement

```bash
# Mensuel ou lorsque performance < seuil
cd backend/src/
python preprocessing_fraud_class.py  # Nouvelles donnÃ©es

cd ../../notebooks/
jupyter nbconvert --execute fraud_detection_modeling_advanced.ipynb
```

### Logs et Alertes

```python
# backend/src/monitoring.py
import logging

logging.basicConfig(
    filename='fraud_detection.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def log_prediction(transaction_id, prediction, probability):
    logging.info(f"ID: {transaction_id} | Pred: {prediction} | Prob: {probability:.4f}")
```

---

## ğŸ”§ Configuration AvancÃ©e

### Personnalisation des HyperparamÃ¨tres

Dans le notebook, cellule "5.1 DÃ©finition des HyperparamÃ¨tres":

```python
param_distributions = {
    'XGBoost': {
        'n_estimators': [100, 200, 300, 400, 500],  # Ajouter plus de valeurs
        'max_depth': [3, 5, 7, 9, 11],
        'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
        # ... personnaliser selon besoins
    }
}
```

### Ajuster le Seuil de DÃ©cision

```python
# Dans model_inference.py ou en post-traitement
optimal_threshold = 0.3  # Plus de dÃ©tections (+ Recall, - Precision)
# ou
optimal_threshold = 0.7  # Moins de fausses alertes (- Recall, + Precision)

result = detector.predict_single(transaction, threshold=optimal_threshold)
```

### Utiliser GridSearchCV (Recherche Exhaustive)

Pour le meilleur modÃ¨le identifiÃ©:

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [280, 300, 320],
    'max_depth': [7, 8, 9],
    'learning_rate': [0.08, 0.1, 0.12]
}

grid_search = GridSearchCV(
    XGBClassifier(...),
    param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)
```

---

## ğŸ› DÃ©pannage

### ProblÃ¨mes Courants

#### 1. `ModuleNotFoundError`

**Cause**: DÃ©pendances non installÃ©es

**Solution**:
```bash
pip install -r requirements.txt
```

#### 2. `FileNotFoundError: preprocessed_data.pkl`

**Cause**: Preprocessing non exÃ©cutÃ©

**Solution**:
```bash
cd backend/src/
python preprocessing_fraud_class.py
```

#### 3. `MemoryError` pendant le fine-tuning

**Cause**: RAM insuffisante

**Solution**: RÃ©duire `n_iter` et `cv`
```python
n_iter=5,   # Au lieu de 20
cv=2        # Au lieu de 3
```

#### 4. Le notebook est trÃ¨s lent

**Solutions**:
- RÃ©duire `n_estimators` dans les modÃ¨les
- Utiliser moins d'itÃ©rations pour RandomizedSearchCV
- Fermer d'autres applications
- Utiliser un Ã©chantillon des donnÃ©es

#### 5. Erreur lors du chargement du modÃ¨le

**Cause**: Versions de packages incompatibles

**Solution**:
```bash
pip install --upgrade scikit-learn xgboost lightgbm catboost
```

---

## ğŸ“š Documentation SupplÃ©mentaire

### Fichiers de Documentation

- **README_MODELING.md**: Documentation dÃ©taillÃ©e du notebook

### Ressources Externes

- [Scikit-learn Documentation](https://scikit-learn.org/stable/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [CatBoost Documentation](https://catboost.ai/docs/)
- [Imbalanced-learn Documentation](https://imbalanced-learn.org/)


---

## ğŸ“ Changelog

### Version 1.0 (Novembre 2025)
- âœ… ImplÃ©mentation complÃ¨te du pipeline
- âœ… 4 modÃ¨les de base + ensembles
- âœ… Fine-tuning automatisÃ©
- âœ… Stacking et Voting
- âœ… Script d'infÃ©rence production-ready
- âœ… Documentation exhaustive

---

## ğŸ‘¥ Ã‰quipe

**Projet MLOps 2025 - 3 IDSD ID**

Made with â¤ï¸ by 3 IDSD ID Team

</div>
