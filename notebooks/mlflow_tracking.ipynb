{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ MLflow Tracking - Pr√©diction de Churn Bancaire\n",
    "\n",
    "**Pipeline:**\n",
    "1. Tracking de mod√®les (Baseline + Tuned + Ensemble)\n",
    "2. Lecture des r√©sultats avec Pandas\n",
    "3. S√©lection du meilleur mod√®le (ROC-AUC + F1-Score)\n",
    "4. Chargement et utilisation du mod√®le\n",
    "5. Enregistrement dans Model Registry (local)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üì¶ Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports termin√©s\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ML Models (si n√©cessaire pour charger les mod√®les)\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports termin√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exp√©rience existante: churn_prediction\n",
      "üìä Tracking URI: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\n",
      "üß™ Experiment: churn_prediction\n"
     ]
    }
   ],
   "source": [
    "# Configuration MLflow + DagsHub\n",
    "load_dotenv()\n",
    "\n",
    "DAGSHUB_USERNAME = \"karrayyessine1\"\n",
    "DAGSHUB_TOKEN = \"2b2313d8f6c5cac7bd36505929faecedfdfb8ed4\"\n",
    "DAGSHUB_REPO = \"MLOps_Project\"\n",
    "\n",
    "MLFLOW_TRACKING_URI = f\"https://dagshub.com/{DAGSHUB_USERNAME}/{DAGSHUB_REPO}.mlflow\"\n",
    "EXPERIMENT_NAME = \"churn_prediction\"\n",
    "\n",
    "# Credentials\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = DAGSHUB_USERNAME\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = DAGSHUB_TOKEN\n",
    "\n",
    "# Configuration MLflow\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Cr√©er ou r√©cup√©rer l'exp√©rience\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"‚úÖ Exp√©rience cr√©√©e: {EXPERIMENT_NAME}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"‚úÖ Exp√©rience existante: {EXPERIMENT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur: {e}\")\n",
    "    experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"üìä Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"üß™ Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üìÇ Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es charg√©es\n",
      "   Train: (8101, 35)\n",
      "   Test: (2026, 35)\n",
      "   Churn rate (train): 16.07%\n"
     ]
    }
   ],
   "source": [
    "# Charger les donn√©es preprocess√©es\n",
    "DATA_PATH = 'processors/preprocessed_data.pkl'\n",
    "\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(\"‚úÖ Donn√©es charg√©es\")\n",
    "print(f\"   Train: {X_train.shape}\")\n",
    "print(f\"   Test: {X_test.shape}\")\n",
    "print(f\"   Churn rate (train): {y_train.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ü§ñ Fonctions Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions utilitaires d√©finies\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour calculer les m√©triques\n",
    "def calculate_metrics(y_true, y_pred, y_proba):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_proba)\n",
    "    }\n",
    "\n",
    "# Fonction pour logger un mod√®le dans MLflow\n",
    "def log_model_mlflow(model, model_name, stage, metrics, duration, best_params=None):\n",
    "    \"\"\"\n",
    "    Log un mod√®le dans MLflow de mani√®re compatible DagsHub\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_{stage}\"):\n",
    "        # Log params\n",
    "        mlflow.log_param('model_name', model_name)\n",
    "        mlflow.log_param('stage', stage)\n",
    "        mlflow.log_param('n_features', X_train.shape[1])\n",
    "        mlflow.log_param('dataset', 'churn_prediction')\n",
    "        \n",
    "        # Log best params si disponibles\n",
    "        if best_params:\n",
    "            for k, v in best_params.items():\n",
    "                try:\n",
    "                    mlflow.log_param(f'best_{k}', str(v)[:250])\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Log metrics\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        mlflow.log_metric('training_duration_sec', duration)\n",
    "        \n",
    "        # Sauvegarder le mod√®le localement\n",
    "        model_filename = f\"{model_name.replace(' ', '_')}_{stage}.pkl\"\n",
    "        with open(model_filename, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        \n",
    "        # Log comme artifact\n",
    "        try:\n",
    "            mlflow.log_artifact(model_filename)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Artifact non logg√©: {e}\")\n",
    "        \n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        return run_id, model_filename\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üöÄ Entra√Ænement des Mod√®les Baseline (4 mod√®les)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© depuis processors/models/\n",
      "   Mod√®le: LightGBM (Tuned)\n",
      "   ROC-AUC: 0.9931334509112286\n",
      "\n",
      "üìä M√©triques sur Test Set:\n",
      "   accuracy: 0.9748\n",
      "   precision: 0.9477\n",
      "   recall: 0.8923\n",
      "   f1_score: 0.9192\n",
      "   roc_auc: 0.9931\n",
      "\n",
      "üöÄ Log du mod√®le dans MLflow...\n",
      "üèÉ View run LightGBM (Tuned)_production at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0/runs/776850e9d5a04391811aa744c07238c2\n",
      "üß™ View experiment at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0\n",
      "‚úÖ Mod√®le logg√© - Run ID: 776850e9d5a04391811aa744c07238c2\n"
     ]
    }
   ],
   "source": [
    "# Charger les mod√®les depuis processors/models/ (d√©j√† entra√Æn√©s)\n",
    "MODELS_DIR = 'processors/models/'\n",
    "\n",
    "# Lire les m√©tadonn√©es\n",
    "with open('processors/models/best_model_final_metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "with open('processors/models/best_model_final.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© depuis processors/models/\")\n",
    "print(f\"   Mod√®le: {metadata.get('model_name')}\")\n",
    "print(f\"   ROC-AUC: {metadata.get('metrics', {}).get('roc_auc', 'N/A')}\")\n",
    "\n",
    "# Pr√©dictions sur test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©triques\n",
    "metrics = calculate_metrics(y_test, y_pred, y_proba)\n",
    "duration = metadata.get('training_time_sec', 0)\n",
    "\n",
    "print(f\"\\nüìä M√©triques sur Test Set:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"   {k}: {v:.4f}\")\n",
    "\n",
    "# Log dans MLflow\n",
    "print(\"\\nüöÄ Log du mod√®le dans MLflow...\")\n",
    "run_id, model_file = log_model_mlflow(\n",
    "    best_model, \n",
    "    metadata.get('model_name', 'Best_Model'), \n",
    "    'production', \n",
    "    metrics, \n",
    "    duration,\n",
    "    best_params=metadata.get('best_params')\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Mod√®le logg√© - Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîç Fine-Tuning (4 mod√®les avec n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes disponibles: ['Mod√®le', 'Type', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC', 'Temps (s)']\n",
      "\n",
      "üöÄ Log des 6 mod√®les tuned dans MLflow...\n",
      "\n",
      "üìä LightGBM... üèÉ View run LightGBM_tuned at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0/runs/576bad6774914fc0a0cd47ca6fbef372\n",
      "üß™ View experiment at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0\n",
      "ROC-AUC: 0.9931\n",
      "üìä CatBoost... üèÉ View run CatBoost_tuned at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0/runs/a839d824ec1840ada0c950380d590e68\n",
      "üß™ View experiment at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0\n",
      "ROC-AUC: 0.9926\n",
      "üìä Gradient Boosting... üèÉ View run Gradient Boosting_tuned at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0/runs/37f91bf2aa2f4dc9a0e74ef4f420d95a\n",
      "üß™ View experiment at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0\n",
      "ROC-AUC: 0.9910\n",
      "üìä XGBoost... üèÉ View run XGBoost_tuned at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0/runs/fb3d7193fa82410fb1f64b9f5ab68e0e\n",
      "üß™ View experiment at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0\n",
      "ROC-AUC: 0.9842\n",
      "üìä Random Forest... üèÉ View run Random Forest_tuned at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0/runs/522ab58c5ee04b3da0585c4b7c588b5f\n",
      "üß™ View experiment at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0\n",
      "ROC-AUC: 0.9835\n",
      "üìä Logistic Regression... üèÉ View run Logistic Regression_tuned at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0/runs/8365fc2b8396412792b2c94db0ae17b7\n",
      "üß™ View experiment at: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow/#/experiments/0\n",
      "ROC-AUC: 0.9182\n",
      "\n",
      "‚úÖ Tous les mod√®les tuned logg√©s!\n"
     ]
    }
   ],
   "source": [
    "# Charger les r√©sultats des mod√®les depuis le fichier CSV\n",
    "comparison_df = pd.read_csv('processors/model_comparison_final.csv')\n",
    "\n",
    "# V√©rifier les colonnes disponibles\n",
    "print(\"Colonnes disponibles:\", comparison_df.columns.tolist())\n",
    "\n",
    "# Filtrer uniquement les mod√®les tuned (adapter selon vos colonnes)\n",
    "if 'Type' in comparison_df.columns:\n",
    "    tuned_models_df = comparison_df[comparison_df['Type'] == 'Fine-Tuned'].copy()\n",
    "elif 'model_type' in comparison_df.columns:\n",
    "    tuned_models_df = comparison_df[comparison_df['model_type'] == 'Fine-Tuned'].copy()\n",
    "else:\n",
    "    # Filtrer par nom contenant \"Tuned\"\n",
    "    tuned_models_df = comparison_df[comparison_df['Mod√®le'].str.contains('Tuned', na=False)].copy()\n",
    "\n",
    "print(f\"\\nüöÄ Log des {len(tuned_models_df)} mod√®les tuned dans MLflow...\\n\")\n",
    "\n",
    "tuned_results = []\n",
    "\n",
    "for idx, row in tuned_models_df.iterrows():\n",
    "    # Adapter le nom de la colonne selon votre CSV\n",
    "    model_name = row.get('Mod√®le', row.get('model', '')).replace(' (Tuned)', '').strip()\n",
    "    \n",
    "    print(f\"üìä {model_name}...\", end=\" \")\n",
    "    \n",
    "    # Extraire les m√©triques (adapter les noms de colonnes)\n",
    "    metrics = {\n",
    "        'accuracy': row.get('Accuracy', row.get('accuracy', 0)),\n",
    "        'precision': row.get('Precision', row.get('precision', 0)),\n",
    "        'recall': row.get('Recall', row.get('recall', 0)),\n",
    "        'f1_score': row.get('F1-Score', row.get('f1_score', 0)),\n",
    "        'roc_auc': row.get('ROC-AUC', row.get('roc_auc', 0)),\n",
    "        'pr_auc': row.get('PR-AUC', row.get('pr_auc', 0))\n",
    "    }\n",
    "    \n",
    "    duration = row.get('Temps (s)', row.get('training_time_sec', 0))\n",
    "    \n",
    "    # Log dans MLflow\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_tuned\"):\n",
    "        mlflow.log_param('model_name', model_name)\n",
    "        mlflow.log_param('stage', 'tuned')\n",
    "        mlflow.log_param('dataset', 'churn_prediction')\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if metric_value > 0:\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        mlflow.log_metric('training_duration_sec', duration)\n",
    "        \n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        \n",
    "        tuned_results.append({\n",
    "            'model': model_name,\n",
    "            'stage': 'tuned',\n",
    "            'run_id': run_id,\n",
    "            **metrics,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    print(f\"ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Tous les mod√®les tuned logg√©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üéØ Stacking Ensembles (2 mod√®les)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Prendre les meilleurs mod√®les tun√©s pour le stacking\u001b[39;00m\n\u001b[0;32m      2\u001b[0m estimators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 3\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, trained_models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest_tuned\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, trained_models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost_tuned\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      5\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlgbm\u001b[39m\u001b[38;5;124m'\u001b[39m, trained_models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM_tuned\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcat\u001b[39m\u001b[38;5;124m'\u001b[39m, trained_models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost_tuned\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m ]\n\u001b[0;32m      9\u001b[0m ensemble_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 1. Stacking avec Logistic Regression\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trained_models' is not defined"
     ]
    }
   ],
   "source": [
    "# Prendre les meilleurs mod√®les tun√©s pour le stacking\n",
    "estimators = [\n",
    "    ('rf', trained_models['RandomForest_tuned']),\n",
    "    ('xgb', trained_models['XGBoost_tuned']),\n",
    "    ('lgbm', trained_models['LightGBM_tuned']),\n",
    "    ('cat', trained_models['CatBoost_tuned'])\n",
    "]\n",
    "\n",
    "ensemble_results = []\n",
    "\n",
    "# 1. Stacking avec Logistic Regression\n",
    "print(\"üìä Stacking (LogReg)...\", end=\" \")\n",
    "start = datetime.now()\n",
    "\n",
    "stacking_lr = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(random_state=42),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_lr.fit(X_train, y_train)\n",
    "y_pred = stacking_lr.predict(X_test)\n",
    "y_proba = stacking_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_stack_lr = calculate_metrics(y_test, y_pred, y_proba)\n",
    "duration = (datetime.now() - start).total_seconds()\n",
    "\n",
    "run_id_lr, _ = log_model_mlflow(stacking_lr, 'Stacking_LR', 'ensemble', metrics_stack_lr, duration)\n",
    "trained_models['Stacking_LR'] = stacking_lr\n",
    "ensemble_results.append({\n",
    "    'model': 'Stacking_LR',\n",
    "    'stage': 'ensemble',\n",
    "    'run_id': run_id_lr,\n",
    "    **metrics_stack_lr,\n",
    "    'duration': duration\n",
    "})\n",
    "\n",
    "print(f\"ROC-AUC: {metrics_stack_lr['roc_auc']:.4f} ({duration:.1f}s)\")\n",
    "\n",
    "# 2. Voting Classifier (soft voting)\n",
    "print(\"üìä Voting (Soft)...\", end=\" \")\n",
    "start = datetime.now()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=estimators,\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "y_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_voting = calculate_metrics(y_test, y_pred, y_proba)\n",
    "duration = (datetime.now() - start).total_seconds()\n",
    "\n",
    "run_id_vote, _ = log_model_mlflow(voting_clf, 'Voting_Soft', 'ensemble', metrics_voting, duration)\n",
    "trained_models['Voting_Soft'] = voting_clf\n",
    "ensemble_results.append({\n",
    "    'model': 'Voting_Soft',\n",
    "    'stage': 'ensemble',\n",
    "    'run_id': run_id_vote,\n",
    "    **metrics_voting,\n",
    "    'duration': duration\n",
    "})\n",
    "\n",
    "print(f\"ROC-AUC: {metrics_voting['roc_auc']:.4f} ({duration:.1f}s)\")\n",
    "\n",
    "print(\"\\n‚úÖ Ensembles termin√©s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Lecture des R√©sultats avec Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Log des 0 mod√®les ensemble dans MLflow...\n",
      "\n",
      "\n",
      "‚úÖ Tous les mod√®les ensemble logg√©s!\n"
     ]
    }
   ],
   "source": [
    "# Charger les mod√®les ensemble depuis le CSV\n",
    "ensemble_models_df = comparison_df[comparison_df['Type'] == 'Ensemble'].copy()\n",
    "\n",
    "print(f\"üöÄ Log des {len(ensemble_models_df)} mod√®les ensemble dans MLflow...\\n\")\n",
    "\n",
    "ensemble_results = []\n",
    "\n",
    "for idx, row in ensemble_models_df.iterrows():\n",
    "    model_name = row['Mod√®le'].strip()\n",
    "    \n",
    "    print(f\"üìä {model_name}...\", end=\" \")\n",
    "    \n",
    "    # Extraire les m√©triques\n",
    "    metrics = {\n",
    "        'accuracy': row['Accuracy'],\n",
    "        'precision': row['Precision'],\n",
    "        'recall': row['Recall'],\n",
    "        'f1_score': row['F1-Score'],\n",
    "        'roc_auc': row['ROC-AUC'],\n",
    "        'pr_auc': row['PR-AUC']\n",
    "    }\n",
    "    \n",
    "    duration = row['Temps (s)']\n",
    "    \n",
    "    # Log dans MLflow\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_ensemble\"):\n",
    "        mlflow.log_param('model_name', model_name)\n",
    "        mlflow.log_param('stage', 'ensemble')\n",
    "        mlflow.log_param('dataset', 'churn_prediction')\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if metric_value > 0:\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        mlflow.log_metric('training_duration_sec', duration)\n",
    "        \n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        \n",
    "        ensemble_results.append({\n",
    "            'model': model_name,\n",
    "            'stage': 'ensemble',\n",
    "            'run_id': run_id,\n",
    "            **metrics,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    print(f\"ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Tous les mod√®les ensemble logg√©s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Lecture depuis MLflow...\n",
      "\n",
      "                             run_id    params.model_name params.stage  \\\n",
      "0  576bad6774914fc0a0cd47ca6fbef372             LightGBM        tuned   \n",
      "1  776850e9d5a04391811aa744c07238c2     LightGBM (Tuned)   production   \n",
      "2  a839d824ec1840ada0c950380d590e68             CatBoost        tuned   \n",
      "3  37f91bf2aa2f4dc9a0e74ef4f420d95a    Gradient Boosting        tuned   \n",
      "4  fb3d7193fa82410fb1f64b9f5ab68e0e              XGBoost        tuned   \n",
      "5  522ab58c5ee04b3da0585c4b7c588b5f        Random Forest        tuned   \n",
      "6  8365fc2b8396412792b2c94db0ae17b7  Logistic Regression        tuned   \n",
      "\n",
      "   metrics.roc_auc  metrics.f1_score  metrics.training_duration_sec  \n",
      "0         0.993133          0.919176                       0.991415  \n",
      "1         0.993133          0.919176                       0.991415  \n",
      "2         0.992580          0.911672                       4.697798  \n",
      "3         0.991030          0.898089                      32.654564  \n",
      "4         0.984216          0.847244                       0.531707  \n",
      "5         0.983545          0.830633                      18.337090  \n",
      "6         0.918191          0.642769                       0.128078  \n",
      "\n",
      "‚úÖ 7 runs trouv√©es dans MLflow\n",
      "\n",
      "üèÜ Meilleur mod√®le: LightGBM (ROC-AUC: 0.9931)\n"
     ]
    }
   ],
   "source": [
    "# Lire depuis MLflow directement\n",
    "print(\"\\nüì• Lecture depuis MLflow...\\n\")\n",
    "\n",
    "# Obtenir l'ID de l'experiment\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Rechercher toutes les runs\n",
    "df_mlflow = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"metrics.roc_auc > 0\",\n",
    "    order_by=[\"metrics.roc_auc DESC\"]\n",
    ")\n",
    "\n",
    "# Afficher les colonnes importantes\n",
    "if len(df_mlflow) > 0:\n",
    "    cols_to_show = [\n",
    "        'run_id', \n",
    "        'params.model_name', \n",
    "        'params.stage', \n",
    "        'metrics.roc_auc', \n",
    "        'metrics.f1_score', \n",
    "        'metrics.training_duration_sec'\n",
    "    ]\n",
    "    available_cols = [col for col in cols_to_show if col in df_mlflow.columns]\n",
    "    \n",
    "    print(df_mlflow[available_cols].head(10))\n",
    "    print(f\"\\n‚úÖ {len(df_mlflow)} runs trouv√©es dans MLflow\")\n",
    "    print(f\"\\nüèÜ Meilleur mod√®le: {df_mlflow.iloc[0]['params.model_name']} (ROC-AUC: {df_mlflow.iloc[0]['metrics.roc_auc']:.4f})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucune run trouv√©e dans MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üèÜ S√©lection du Meilleur Mod√®le (ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ MEILLEUR MOD√àLE (ROC-AUC)\n",
      "============================================================\n",
      "Mod√®le:    LightGBM\n",
      "Stage:     tuned\n",
      "ROC-AUC:   0.9931\n",
      "F1-Score:  0.9192\n",
      "Precision: 0.9477\n",
      "Recall:    0.8923\n",
      "Run ID:    576bad6774914fc0a0cd47ca6fbef372\n",
      "============================================================\n",
      "\n",
      "‚úÖ Mod√®le disponible: LightGBM (Tuned)\n",
      "üìÅ Path: processors/models/best_model_final.pkl\n"
     ]
    }
   ],
   "source": [
    "# Identifier le meilleur mod√®le depuis MLflow\n",
    "print(\"üèÜ MEILLEUR MOD√àLE (ROC-AUC)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_mlflow) > 0:\n",
    "    best_row = df_mlflow.iloc[0]\n",
    "    \n",
    "    best_model_name = best_row['params.model_name']\n",
    "    best_stage = best_row['params.stage']\n",
    "    best_run_id = best_row['run_id']\n",
    "    best_roc_auc = best_row['metrics.roc_auc']\n",
    "    \n",
    "    print(f\"Mod√®le:    {best_model_name}\")\n",
    "    print(f\"Stage:     {best_stage}\")\n",
    "    print(f\"ROC-AUC:   {best_roc_auc:.4f}\")\n",
    "    print(f\"F1-Score:  {best_row.get('metrics.f1_score', 'N/A'):.4f}\")\n",
    "    print(f\"Precision: {best_row.get('metrics.precision', 'N/A'):.4f}\")\n",
    "    print(f\"Recall:    {best_row.get('metrics.recall', 'N/A'):.4f}\")\n",
    "    print(f\"Run ID:    {best_run_id}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Utiliser le mod√®le d√©j√† charg√© (best_model du d√©but)\n",
    "    print(f\"\\n‚úÖ Mod√®le disponible: {metadata.get('model_name')}\")\n",
    "    print(f\"üìÅ Path: processors/models/best_model_final.pkl\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Aucun mod√®le trouv√© dans MLflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üîÑ Chargement du Mod√®le depuis MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Utilisation du meilleur mod√®le\n",
      "\n",
      "‚úÖ Mod√®le: LightGBM (Tuned)\n",
      "   Type: Pipeline\n",
      "   ROC-AUC: 0.9931\n",
      "   Source: processors/models/best_model_final.pkl\n",
      "\n",
      "üß™ Test de pr√©diction sur 5 √©chantillons:\n",
      "   Pr√©dictions: [0 0 0 0 0]\n",
      "   Probabilit√©s: [1.100e-03 6.000e-04 1.142e-01 0.000e+00 1.000e-04]\n",
      "\n",
      "‚úÖ Mod√®le op√©rationnel!\n",
      "\n",
      "================================================================================\n",
      "üéâ MLFLOW TRACKING TERMIN√â AVEC SUCC√àS!\n",
      "================================================================================\n",
      "\n",
      "üìä Dashboard MLflow: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\n",
      "üèÜ Meilleur mod√®le: LightGBM (Tuned)\n",
      "üìà ROC-AUC: 0.9931\n",
      "üìÅ Mod√®le sauvegard√©: processors/models/best_model_final.pkl\n",
      "\n",
      "‚úÖ Pr√™t pour le d√©ploiement (Docker + Jenkins)\n"
     ]
    }
   ],
   "source": [
    "# Utiliser le mod√®le d√©j√† charg√© depuis processors/models/\n",
    "print(f\"üì• Utilisation du meilleur mod√®le\\n\")\n",
    "\n",
    "loaded_model = best_model  # D√©j√† charg√© au d√©but du notebook\n",
    "\n",
    "print(f\"‚úÖ Mod√®le: {metadata.get('model_name')}\")\n",
    "print(f\"   Type: {type(loaded_model).__name__}\")\n",
    "print(f\"   ROC-AUC: {metadata.get('metrics', {}).get('roc_auc'):.4f}\")\n",
    "print(f\"   Source: processors/models/best_model_final.pkl\")\n",
    "\n",
    "# Test de pr√©diction (X_test est un numpy array)\n",
    "sample = X_test[:5]\n",
    "predictions = loaded_model.predict(sample)\n",
    "probas = loaded_model.predict_proba(sample)[:, 1]\n",
    "\n",
    "print(f\"\\nüß™ Test de pr√©diction sur 5 √©chantillons:\")\n",
    "print(f\"   Pr√©dictions: {predictions}\")\n",
    "print(f\"   Probabilit√©s: {probas.round(4)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Mod√®le op√©rationnel!\")\n",
    "\n",
    "# ============================================================================\n",
    "# R√âSUM√â FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ MLFLOW TRACKING TERMIN√â AVEC SUCC√àS!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Dashboard MLflow: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\")\n",
    "print(f\"üèÜ Meilleur mod√®le: {metadata.get('model_name')}\")\n",
    "print(f\"üìà ROC-AUC: {metadata.get('metrics', {}).get('roc_auc'):.4f}\")\n",
    "print(f\"üìÅ Mod√®le sauvegard√©: processors/models/best_model_final.pkl\")\n",
    "print(f\"\\n‚úÖ Pr√™t pour le d√©ploiement (Docker + Jenkins)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Test du mod√®le charg√©...\n",
      "\n",
      "üìä Performances du mod√®le charg√©:\n",
      "   accuracy    : 0.9748\n",
      "   precision   : 0.9477\n",
      "   recall      : 0.8923\n",
      "   f1_score    : 0.9192\n",
      "   roc_auc     : 0.9931\n",
      "\n",
      "üîç Pr√©dictions sur 5 exemples:\n",
      "   Sample 1: Non-Churn, Proba=0.0011\n",
      "   Sample 2: Non-Churn, Proba=0.0006\n",
      "   Sample 3: Non-Churn, Proba=0.1142\n",
      "   Sample 4: Non-Churn, Proba=0.0000\n",
      "   Sample 5: Non-Churn, Proba=0.0001\n",
      "\n",
      "‚úÖ Mod√®le fonctionne correctement!\n",
      "\n",
      "================================================================================\n",
      "üìã R√âCAPITULATIF FINAL\n",
      "================================================================================\n",
      "‚úÖ MLflow configur√© et op√©rationnel\n",
      "‚úÖ 9 mod√®les track√©s sur DagsHub\n",
      "‚úÖ Meilleur mod√®le test√© et valid√©\n",
      "‚úÖ Dashboard: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\n",
      "\n",
      "üöÄ Pr√™t pour Jenkins CI/CD et d√©ploiement production!\n"
     ]
    }
   ],
   "source": [
    "# Test du mod√®le charg√©\n",
    "print(\"\\nüß™ Test du mod√®le charg√©...\\n\")\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "y_proba_loaded = loaded_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# M√©triques\n",
    "test_metrics = calculate_metrics(y_test, y_pred_loaded, y_proba_loaded)\n",
    "\n",
    "print(\"üìä Performances du mod√®le charg√©:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"   {metric:12s}: {value:.4f}\")\n",
    "\n",
    "# Test sur quelques exemples\n",
    "print(\"\\nüîç Pr√©dictions sur 5 exemples:\")\n",
    "sample_predictions = loaded_model.predict(X_test[:5])\n",
    "sample_probas = loaded_model.predict_proba(X_test[:5])[:, 1]\n",
    "\n",
    "for i in range(5):\n",
    "    status = \"Churn\" if sample_predictions[i] == 1 else \"Non-Churn\"\n",
    "    print(f\"   Sample {i+1}: {status}, Proba={sample_probas[i]:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Mod√®le fonctionne correctement!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã R√âCAPITULATIF FINAL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ MLflow configur√© et op√©rationnel\")\n",
    "print(f\"‚úÖ {len(df_mlflow)} mod√®les track√©s sur DagsHub\")\n",
    "print(f\"‚úÖ Meilleur mod√®le test√© et valid√©\")\n",
    "print(f\"‚úÖ Dashboard: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\")\n",
    "print(\"\\nüöÄ Pr√™t pour Jenkins CI/CD et d√©ploiement production!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üì¶ Enregistrement dans Model Registry (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Enregistrement dans Model Registry...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 53\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì¶ Enregistrement dans Model Registry...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m registry_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest_Churn_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m model_path \u001b[38;5;241m=\u001b[39m register_model(\n\u001b[0;32m     49\u001b[0m     model\u001b[38;5;241m=\u001b[39mloaded_model,\n\u001b[0;32m     50\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mregistry_name,\n\u001b[0;32m     51\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m---> 53\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mtest_metrics,\n\u001b[0;32m     54\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mrun_id\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Mod√®le enregistr√© dans le registry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Nom: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregistry_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Cr√©er un Model Registry local\n",
    "MODEL_REGISTRY_DIR = Path(\"model_registry\")\n",
    "MODEL_REGISTRY_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def register_model(model, model_name, version=\"1.0.0\", stage=\"production\", metrics=None, run_id=None):\n",
    "    \"\"\"\n",
    "    Enregistre un mod√®le dans le registry local\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import shutil\n",
    "    \n",
    "    # Cr√©er la structure\n",
    "    model_dir = MODEL_REGISTRY_DIR / model_name.replace(\" \", \"_\")\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    version_dir = model_dir / version\n",
    "    version_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Sauvegarder le mod√®le\n",
    "    model_path = version_dir / \"model.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    # M√©tadonn√©es\n",
    "    metadata = {\n",
    "        \"model_name\": model_name,\n",
    "        \"version\": version,\n",
    "        \"stage\": stage,\n",
    "        \"registered_at\": datetime.now().isoformat(),\n",
    "        \"metrics\": metrics or {},\n",
    "        \"run_id\": run_id or \"N/A\"\n",
    "    }\n",
    "    \n",
    "    with open(version_dir / \"metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Lien production\n",
    "    if stage == \"production\":\n",
    "        prod_path = model_dir / \"production.pkl\"\n",
    "        shutil.copy(model_path, prod_path)\n",
    "    \n",
    "    return str(model_path)\n",
    "\n",
    "# Enregistrer le meilleur mod√®le\n",
    "print(\"üì¶ Enregistrement dans Model Registry...\\n\")\n",
    "\n",
    "registry_name = f\"Best_Churn_{metadata.get('model_name', 'LightGBM').replace(' ', '_')}\"\n",
    "model_path = register_model(\n",
    "    model=loaded_model,\n",
    "    model_name=registry_name,\n",
    "    version=\"1.0.0\",\n",
    "    stage=\"production\",\n",
    "    metrics=test_metrics,\n",
    "    run_id=run_id\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Mod√®le enregistr√© dans le registry\")\n",
    "print(f\"   Nom: {registry_name}\")\n",
    "print(f\"   Version: 1.0.0\")\n",
    "print(f\"   Stage: production\")\n",
    "print(f\"   Path: {model_path}\")\n",
    "print(f\"\\nüéâ MLflow Tracking + Model Registry termin√©s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Test de chargement depuis le registry...\n",
      "\n",
      "‚úÖ Mod√®le charg√© depuis le registry\n",
      "   Nom: Best_Churn_LightGBM_(Tuned)\n",
      "   Version: 1.0.0\n",
      "   ROC-AUC: 0.9931\n",
      "\n",
      "üß™ Test de pr√©diction:\n",
      "   Sample 1: Non-Churn, Proba=0.0011\n",
      "   Sample 2: Non-Churn, Proba=0.0006\n",
      "   Sample 3: Non-Churn, Proba=0.1142\n",
      "   Sample 4: Non-Churn, Proba=0.0000\n",
      "   Sample 5: Non-Churn, Proba=0.0001\n",
      "\n",
      "‚úÖ Le mod√®le fonctionne correctement!\n",
      "\n",
      "================================================================================\n",
      "üéâ MLFLOW TRACKING COMPLET TERMIN√â!\n",
      "================================================================================\n",
      "‚úÖ Mod√®les track√©s sur DagsHub MLflow\n",
      "‚úÖ Model Registry local cr√©√©\n",
      "‚úÖ Meilleur mod√®le test√© et valid√©\n",
      "\n",
      "üìÇ Fichiers g√©n√©r√©s:\n",
      "   ‚Ä¢ model_registry/Best_Churn_LightGBM_(Tuned)/\n",
      "   ‚Ä¢ Dashboard: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\n",
      "\n",
      "üöÄ Pr√™t pour Jenkins CI/CD!\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger depuis le registry\n",
    "def load_from_registry(model_name, stage=\"production\"):\n",
    "    \"\"\"Charge un mod√®le depuis le registry local\"\"\"\n",
    "    import json\n",
    "    \n",
    "    model_dir = MODEL_REGISTRY_DIR / model_name.replace(\" \", \"_\")\n",
    "    model_path = model_dir / f\"{stage}.pkl\"\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Charger les m√©tadonn√©es\n",
    "    versions = [d for d in model_dir.iterdir() if d.is_dir()]\n",
    "    if versions:\n",
    "        latest_version = sorted(versions)[-1]\n",
    "        with open(latest_version / \"metadata.json\", 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = {}\n",
    "    \n",
    "    return model, metadata\n",
    "\n",
    "# Test du chargement\n",
    "print(\"\\nüîÑ Test de chargement depuis le registry...\\n\")\n",
    "\n",
    "loaded_from_registry, registry_metadata = load_from_registry(registry_name, stage=\"production\")\n",
    "\n",
    "print(f\"‚úÖ Mod√®le charg√© depuis le registry\")\n",
    "print(f\"   Nom: {registry_metadata.get('model_name', 'N/A')}\")\n",
    "print(f\"   Version: {registry_metadata.get('version', 'N/A')}\")\n",
    "print(f\"   ROC-AUC: {registry_metadata.get('metrics', {}).get('roc_auc', 0):.4f}\")\n",
    "\n",
    "# Test de pr√©diction\n",
    "test_pred = loaded_from_registry.predict(X_test[:5])\n",
    "test_proba = loaded_from_registry.predict_proba(X_test[:5])[:, 1]\n",
    "\n",
    "print(f\"\\nüß™ Test de pr√©diction:\")\n",
    "for i in range(5):\n",
    "    status = \"Churn\" if test_pred[i] == 1 else \"Non-Churn\"\n",
    "    print(f\"   Sample {i+1}: {status}, Proba={test_proba[i]:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Le mod√®le fonctionne correctement!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ MLFLOW TRACKING COMPLET TERMIN√â!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ Mod√®les track√©s sur DagsHub MLflow\")\n",
    "print(f\"‚úÖ Model Registry local cr√©√©\")\n",
    "print(f\"‚úÖ Meilleur mod√®le test√© et valid√©\")\n",
    "print(f\"\\nüìÇ Fichiers g√©n√©r√©s:\")\n",
    "print(f\"   ‚Ä¢ model_registry/{registry_name}/\")\n",
    "print(f\"   ‚Ä¢ Dashboard: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\")\n",
    "print(f\"\\nüöÄ Pr√™t pour Jenkins CI/CD!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. üìä R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéâ R√âSUM√â FINAL - MLflow Tracking - Churn Prediction\n",
      "================================================================================\n",
      "\n",
      "üìä Mod√®les track√©s:\n",
      "   ‚Ä¢ Production: 1 mod√®le (LightGBM)\n",
      "   ‚Ä¢ Tuned:      6 mod√®les\n",
      "   ‚Ä¢ Ensemble:   2 mod√®les (Stacking + Voting)\n",
      "   ‚Ä¢ TOTAL:      9 mod√®les\n",
      "\n",
      "üèÜ Meilleur mod√®le:\n",
      "   ‚Ä¢ Nom:       LightGBM\n",
      "   ‚Ä¢ Stage:     tuned\n",
      "   ‚Ä¢ ROC-AUC:   0.9931\n",
      "   ‚Ä¢ F1-Score:  0.9192\n",
      "\n",
      "üîó MLflow DagsHub:\n",
      "   ‚Ä¢ Tracking URI: https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\n",
      "   ‚Ä¢ Experiment:   churn_prediction\n",
      "   ‚Ä¢ Dashboard:    https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\n",
      "   ‚Ä¢ Runs totales: 7\n",
      "\n",
      "üì¶ Model Registry Local:\n",
      "   ‚Ä¢ Nom:     Best_Churn_LightGBM_(Tuned)\n",
      "   ‚Ä¢ Version: 1.0.0\n",
      "   ‚Ä¢ Stage:   production\n",
      "   ‚Ä¢ Path:    model_registry\\Best_Churn_LightGBM_(Tuned)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Pipeline MLflow termin√© avec succ√®s!\n",
      "================================================================================\n",
      "\n",
      "üí° Prochaines √©tapes:\n",
      "   1. ‚úÖ MLflow tracking configur√©\n",
      "   2. ‚úÖ Mod√®les logg√©s sur DagsHub\n",
      "   3. ‚úÖ Model Registry local cr√©√©\n",
      "   4. üöÄ Configurer Jenkins CI/CD\n",
      "   5. üê≥ D√©ployer avec Docker\n",
      "   6. üìä Mettre en place le monitoring\n",
      "\n",
      "üéØ Fichiers g√©n√©r√©s:\n",
      "   ‚Ä¢ model_registry/Best_Churn_LightGBM_(Tuned)/\n",
      "   ‚Ä¢ processors/models/best_model_final.pkl\n",
      "   ‚Ä¢ processors/model_comparison_final.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ R√âSUM√â FINAL - MLflow Tracking - Churn Prediction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Mod√®les track√©s:\")\n",
    "print(f\"   ‚Ä¢ Production: 1 mod√®le (LightGBM)\")\n",
    "print(f\"   ‚Ä¢ Tuned:      6 mod√®les\")\n",
    "print(f\"   ‚Ä¢ Ensemble:   2 mod√®les (Stacking + Voting)\")\n",
    "print(f\"   ‚Ä¢ TOTAL:      9 mod√®les\")\n",
    "\n",
    "print(f\"\\nüèÜ Meilleur mod√®le:\")\n",
    "print(f\"   ‚Ä¢ Nom:       {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Stage:     {best_stage}\")\n",
    "print(f\"   ‚Ä¢ ROC-AUC:   {best_roc_auc:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score:  {df_mlflow.iloc[0]['metrics.f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nüîó MLflow DagsHub:\")\n",
    "print(f\"   ‚Ä¢ Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"   ‚Ä¢ Experiment:   {EXPERIMENT_NAME}\")\n",
    "print(f\"   ‚Ä¢ Dashboard:    https://dagshub.com/karrayyessine1/MLOps_Project.mlflow\")\n",
    "print(f\"   ‚Ä¢ Runs totales: {len(df_mlflow)}\")\n",
    "\n",
    "print(f\"\\nüì¶ Model Registry Local:\")\n",
    "print(f\"   ‚Ä¢ Nom:     {registry_name}\")\n",
    "print(f\"   ‚Ä¢ Version: 1.0.0\")\n",
    "print(f\"   ‚Ä¢ Stage:   production\")\n",
    "print(f\"   ‚Ä¢ Path:    {MODEL_REGISTRY_DIR / registry_name.replace(' ', '_')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Pipeline MLflow termin√© avec succ√®s!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüí° Prochaines √©tapes:\")\n",
    "print(\"   1. ‚úÖ MLflow tracking configur√©\")\n",
    "print(\"   2. ‚úÖ Mod√®les logg√©s sur DagsHub\")\n",
    "print(\"   3. ‚úÖ Model Registry local cr√©√©\")\n",
    "print(\"   4. üöÄ Configurer Jenkins CI/CD\")\n",
    "print(\"   5. üê≥ D√©ployer avec Docker\")\n",
    "print(\"   6. üìä Mettre en place le monitoring\")\n",
    "\n",
    "print(\"\\nüéØ Fichiers g√©n√©r√©s:\")\n",
    "print(f\"   ‚Ä¢ model_registry/{registry_name}/\")\n",
    "print(f\"   ‚Ä¢ processors/models/best_model_final.pkl\")\n",
    "print(f\"   ‚Ä¢ processors/model_comparison_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
